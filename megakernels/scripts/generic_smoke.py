#!/usr/bin/env python3
import argparse
import math
import sys


def require_torch():
    try:
        import torch  # noqa: F401
    except Exception:
        print("ERROR: This smoke test requires PyTorch. Please install torch and try again.")
        sys.exit(2)


def require_extension():
    try:
        import importlib
        importlib.import_module("mk_generic")
    except Exception as e:
        print("ERROR: mk_generic extension not found. Build it with:\n"
              "  export THUNDERKITTENS_ROOT=$(pwd)/ThunderKittens\n"
              "  export MEGAKERNELS_ROOT=$(pwd)\n"
              "  export PYTHON_VERSION=$(python3 -c 'import sys;print(f\"{sys.version_info[0]}.{sys.version_info[1]}\")')\n"
              "  export GPU=H100  # or 4090/A100; unset for B200\n"
              "  cd demos/generic-hopper && make && cd ../..\n"
              f"Details: {e}")
        sys.exit(3)


def run_matmul(benchmark=False):
    import torch
    import importlib
    mk_generic = importlib.import_module("mk_generic")
    device = torch.device("cuda")

    # opcode=0x30: MATMUL, m=1, n=4, k=3
    inst = torch.zeros(32, dtype=torch.int32)
    inst[0] = (0x30)
    inst[1] = (1) | (4 << 16)
    inst[2] = (3)
    inst[4] = 0  # a offset
    inst[8] = 0  # b (weight) offset
    inst[7] = 0  # c offset

    instructions = inst.view(1, 1, 32).contiguous().to(device)
    timings = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

    a = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32, device=device)
    b = torch.tensor([
        1.0, 0.0, 0.0,
        0.0, 1.0, 0.0,
        0.0, 0.0, 1.0,
        1.0, 1.0, 1.0,
    ], dtype=torch.float32, device=device)
    c = torch.zeros(4, dtype=torch.float32, device=device)
    barriers = torch.zeros(256, dtype=torch.int32, device=device)

    # Warmup run
    mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)

    # Timed run
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)

    num_runs = 100 if benchmark else 10
    start.record()
    for _ in range(num_runs):
        mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)
    end.record()
    torch.cuda.synchronize()

    elapsed_ms = start.elapsed_time(end) / num_runs

    expected = torch.tensor([1.0, 2.0, 3.0, 6.0], device=device)
    ok = torch.allclose(c, expected, atol=1e-6)
    print(f"MATMUL smoke: {'PASS' if ok else 'FAIL'}; got {c.cpu().tolist()}")
    print(f"  Average time: {elapsed_ms:.6f} ms ({num_runs} runs)")

    return 0 if ok else 1


def run_rmsnorm():
    import torch
    import importlib
    mk_generic = importlib.import_module("mk_generic")
    device = torch.device("cuda")

    n = 4
    inst = torch.zeros(32, dtype=torch.int32)
    inst[0] = (0x50)  # RMS_NORM
    inst[1] = (1) | (n << 16)
    inst[4] = 0  # x offset in a
    inst[8] = n  # gamma offset in b (we pack after zeros)
    inst[7] = 0  # y offset in c
    inst[15] = torch.tensor(1e-5, dtype=torch.float32).view(torch.int32)

    instructions = inst.view(1, 1, 32).contiguous().to(device)
    timings = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

    x = torch.tensor([1.0, 2.0, 3.0, 4.0], device=device)
    gamma = torch.ones(n, dtype=torch.float32, device=device)
    a = x.clone()
    b = torch.cat([torch.zeros(n, device=device), gamma])
    c = torch.zeros(n, device=device)
    barriers = torch.zeros(256, dtype=torch.int32, device=device)

    mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)
    rms = torch.sqrt((x.pow(2).mean()) + 1e-5)
    expected = x / rms
    ok = torch.allclose(c, expected, atol=1e-5)
    print(f"RMS_NORM smoke: {'PASS' if ok else 'FAIL'}; got {c.cpu().tolist()}")
    return 0 if ok else 1


def run_attention():
    import torch
    import importlib
    mk_generic = importlib.import_module("mk_generic")
    device = torch.device("cuda")

    head_dim = 3
    kv_len = 2
    inst = torch.zeros(32, dtype=torch.int32)
    inst[0] = (0x70)  # ATTENTION_PARTIAL
    inst[2] = (head_dim)
    inst[10] = (0) | (kv_len << 16)
    inst[4] = 0  # q in a
    inst[5] = 0  # k in b
    inst[6] = head_dim * kv_len  # v in b after k
    inst[7] = 0
    inst[15] = torch.tensor(1.0, dtype=torch.float32).view(torch.int32)

    instructions = inst.view(1, 1, 32).contiguous().to(device)
    timings = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

    q = torch.tensor([1.0, 0.0, 0.0], device=device)
    K = torch.tensor([
        1.0, 0.0, 0.0,
        0.0, 1.0, 0.0,
    ], device=device)
    V = torch.tensor([
        10.0, 0.0, 0.0,
        0.0, 20.0, 0.0,
    ], device=device)
    a = q.clone()
    b = torch.cat([K, V])
    c = torch.zeros(head_dim, device=device)
    barriers = torch.zeros(256, dtype=torch.int32, device=device)

    mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)
    logits = torch.tensor([1.0, 0.0], device=device)
    w = torch.softmax(logits, dim=0)
    expected = w[0] * V[:head_dim] + w[1] * V[head_dim:]
    ok = torch.allclose(c, expected, atol=1e-5)
    print(f"ATTENTION_PARTIAL smoke: {'PASS' if ok else 'FAIL'}; got {c.cpu().tolist()}")
    return 0 if ok else 1


def run_rope():
    import torch
    import importlib
    mk_generic = importlib.import_module("mk_generic")
    device = torch.device("cuda")

    inst = torch.zeros(32, dtype=torch.int32)
    inst[0] = (0x72)  # ROPE_EMBED
    inst[2] = (2)     # head_dim=2
    inst[4] = 0
    inst[7] = 0
    inst[15] = torch.tensor(math.pi/2, dtype=torch.float32).view(torch.int32)

    instructions = inst.view(1, 1, 32).contiguous().to(device)
    timings = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

    x = torch.tensor([2.0, 3.0], device=device)
    a = x.clone()
    b = torch.zeros(1, device=device)
    c = torch.zeros(2, device=device)
    barriers = torch.zeros(256, dtype=torch.int32, device=device)

    mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)
    expected = torch.tensor([-3.0, 2.0], device=device)
    ok = torch.allclose(c, expected, atol=1e-5)
    print(f"ROPE_EMBED smoke: {'PASS' if ok else 'FAIL'}; got {c.cpu().tolist()}")
    return 0 if ok else 1


def run_fused_norm_matmul():
    import torch
    import importlib
    mk_generic = importlib.import_module("mk_generic")
    device = torch.device("cuda")

    # Dimensions: K=3, N=2
    K, N = 3, 2

    # Instruction: OP_FUSED_NORM_MATMUL (0xB0)
    inst = torch.zeros(32, dtype=torch.int32)
    inst[0] = (0xB0)
    inst[1] = (1) | (N << 16)  # m=1, n=N
    inst[2] = (K)              # k_dim=K
    inst[4] = 0  # x in a
    inst[5] = 0  # gamma in b
    inst[8] = K  # W in b after gamma
    inst[7] = 0  # y in c
    inst[15] = torch.tensor(0.0, dtype=torch.float32).view(torch.int32)  # eps

    instructions = inst.view(1, 1, 32).contiguous().to(device)
    timings = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

    x = torch.tensor([1.0, 2.0, 3.0], device=device)
    # Choose gamma so that norm(x)*gamma == x (gamma = rms(x))
    rms = torch.sqrt((x.pow(2).mean()))
    gamma = torch.tensor([rms.item(), rms.item(), rms.item()], device=device)
    W = torch.tensor([
        1.0, 0.0, 0.0,
        1.0, 1.0, 1.0,
    ], device=device)

    a = x.clone()
    b = torch.cat([gamma, W])
    c = torch.zeros(N, device=device)
    barriers = torch.zeros(256, dtype=torch.int32, device=device)

    mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)

    expected = torch.tensor([
        x[0].item(),
        (x[0] + x[1] + x[2]).item(),
    ], device=device)
    ok = torch.allclose(c, expected, atol=1e-6)
    print(f"FUSED_NORM_MATMUL smoke: {'PASS' if ok else 'FAIL'}; got {c.cpu().tolist()}")
    return 0 if ok else 1


def run_fused_qkv_rope():
    import torch
    import importlib
    mk_generic = importlib.import_module("mk_generic")
    device = torch.device("cuda")

    # For now, our OP_FUSED_NORM_QKV_ROPE behaves like fused norm+matmul in smoke
    K, N = 3, 5
    inst = torch.zeros(32, dtype=torch.int32)
    inst[0] = (0xB5)
    inst[1] = (1) | (N << 16)
    inst[2] = (K)
    inst[4] = 0
    inst[5] = 0
    inst[8] = K
    inst[7] = 0
    inst[15] = torch.tensor(0.0, dtype=torch.float32).view(torch.int32)

    instructions = inst.view(1, 1, 32).contiguous().to(device)
    timings = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

    x = torch.tensor([1.0, 2.0, -1.0], device=device)
    gamma = torch.tensor([1.0, 0.5, 2.0], device=device)
    W = torch.tensor([
        1.0, 0.0, 0.0,
        0.0, 1.0, 0.0,
        0.0, 0.0, 1.0,
        1.0, 1.0, 1.0,
        2.0, -1.0, 0.0,
    ], device=device)

    a = x.clone()
    b = torch.cat([gamma, W])
    c = torch.zeros(N, device=device)
    barriers = torch.zeros(256, dtype=torch.int32, device=device)

    mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)

    rms = torch.sqrt((x.pow(2).mean()))
    xn = x / rms * gamma
    Wm = W.view(N, K)
    expected = (Wm @ xn).contiguous()
    ok = torch.allclose(c, expected, atol=1e-6)
    print(f"FUSED_NORM_QKV_ROPE smoke: {'PASS' if ok else 'FAIL'}; got {c.cpu().tolist()}")
    return 0 if ok else 1


def run_barrier():
    import torch
    import importlib
    mk_generic = importlib.import_module("mk_generic")
    device = torch.device("cuda")

    # Barrier instruction (opcode=0x01)
    # This is a synchronization primitive - we test that it completes without hanging
    inst = torch.zeros(32, dtype=torch.int32)
    inst[0] = (0x01)  # OP_BARRIER
    # flags = condition (0=EQUAL)
    inst[0] |= (0 << 8)  # condition in flags lower bits
    inst[2] = 0  # barrier_id = 0 (in input_offset_0)
    inst[3] = 1  # expected_count = 1 (in input_offset_1)

    instructions = inst.view(1, 1, 32).contiguous().to(device)
    timings = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

    a = torch.zeros(4, device=device)
    b = torch.zeros(4, device=device)
    c = torch.zeros(4, device=device)
    barriers = torch.ones(256, dtype=torch.int32, device=device)  # Pre-set barrier[0] = 1

    # Run barrier instruction
    mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)

    # Barrier should increment barrier counter
    # Starting value was 1, after barrier arrives it should be 2
    ok = barriers[0].item() >= 1  # Should not hang, barrier satisfied
    print(f"BARRIER smoke: {'PASS' if ok else 'FAIL'}; barrier[0] = {barriers[0].item()}")
    return 0 if ok else 1


def run_copy():
    import torch
    import importlib
    mk_generic = importlib.import_module("mk_generic")
    device = torch.device("cuda")

    # Copy instruction (opcode=0x15)
    # Copy 4 floats from offset 0 to offset 4
    inst = torch.zeros(32, dtype=torch.int32)
    inst[0] = (0x15)  # OP_COPY
    inst[0] |= (0 << 8)  # flags: dtype=0 (FP32)
    inst[1] = (0) | (4 << 16)  # m_dim=0, n_dim=4 (num_elements)
    inst[4] = 0  # input_offset_0 (source)
    inst[7] = 0  # output_offset (destination in c)

    instructions = inst.view(1, 1, 32).contiguous().to(device)
    timings = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

    src_data = torch.tensor([1.0, 2.0, 3.0, 4.0], device=device)
    a = src_data.clone()
    b = torch.zeros(4, device=device)
    c = torch.zeros(4, device=device)
    barriers = torch.zeros(256, dtype=torch.int32, device=device)

    mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)

    expected = src_data.clone()
    ok = torch.allclose(c, expected, atol=1e-6)
    print(f"COPY smoke: {'PASS' if ok else 'FAIL'}; got {c.cpu().tolist()}")
    return 0 if ok else 1


def run_zero():
    import torch
    import importlib
    mk_generic = importlib.import_module("mk_generic")
    device = torch.device("cuda")

    # Zero instruction (opcode=0x16)
    # Zero 4 floats at offset 0
    inst = torch.zeros(32, dtype=torch.int32)
    inst[0] = (0x16)  # OP_ZERO
    inst[0] |= (0 << 8)  # flags: dtype=0 (FP32)
    inst[1] = (0) | (4 << 16)  # m_dim=0, n_dim=4 (num_elements)
    inst[7] = 0  # output_offset (destination in c)

    instructions = inst.view(1, 1, 32).contiguous().to(device)
    timings = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

    a = torch.zeros(4, device=device)
    b = torch.zeros(4, device=device)
    c = torch.tensor([9.9, 8.8, 7.7, 6.6], device=device)  # Non-zero initial values
    barriers = torch.zeros(256, dtype=torch.int32, device=device)

    mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)

    expected = torch.zeros(4, device=device)
    ok = torch.allclose(c, expected, atol=1e-6)
    print(f"ZERO smoke: {'PASS' if ok else 'FAIL'}; got {c.cpu().tolist()}")
    return 0 if ok else 1


def run_attention_reduce():
    import torch
    import importlib
    mk_generic = importlib.import_module("mk_generic")
    device = torch.device("cuda")

    head_dim = 3
    num_partials = 2
    kv_len_per_partial = 2

    # Query
    q = torch.tensor([1.0, 0.0, 0.0], device=device)

    # First partial: KV pairs 0-1
    K1 = torch.tensor([
        1.0, 0.0, 0.0,  # KV pair 0
        0.5, 0.5, 0.0,  # KV pair 1
    ], device=device)
    V1 = torch.tensor([
        10.0, 0.0, 0.0,
        5.0, 5.0, 0.0,
    ], device=device)

    # Second partial: KV pairs 2-3
    K2 = torch.tensor([
        0.0, 1.0, 0.0,  # KV pair 2
        -0.5, 0.5, 0.0, # KV pair 3
    ], device=device)
    V2 = torch.tensor([
        0.0, 20.0, 0.0,
        -5.0, 5.0, 0.0,
    ], device=device)

    # ========== Run Partial 1 ==========
    inst1 = torch.zeros(32, dtype=torch.int32)
    inst1[0] = (0x70) | (0x01 << 8)  # OP_ATTENTION_PARTIAL with FLAG_PARTIAL
    inst1[2] = (head_dim)  # k_dim = head_dim
    inst1[10] = (0) | (kv_len_per_partial << 16)  # reduction_factor = kv_len
    inst1[4] = 0  # q offset
    inst1[5] = 0  # k offset (in b)
    inst1[6] = kv_len_per_partial * head_dim  # v offset (after K in b)
    inst1[7] = 0  # output offset (in c)
    inst1[9] = head_dim  # scratch_offset for LSE values
    inst1[15] = torch.tensor(1.0, dtype=torch.float32).view(torch.int32)

    instructions1 = inst1.view(1, 1, 32).contiguous().to(device)
    timings1 = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

    a1 = q.clone()
    b1 = torch.cat([K1, V1])
    c1 = torch.zeros(head_dim + 2 + num_partials * (head_dim + 2), device=device)  # Space for outputs + LSE
    barriers1 = torch.zeros(256, dtype=torch.int32, device=device)

    mk_generic.mk_generic_matmul(instructions1, timings1, a1, b1, c1, barriers1)

    o1 = c1[:head_dim].clone()
    lse1 = c1[head_dim:head_dim+2].clone()

    # ========== Run Partial 2 ==========
    inst2 = torch.zeros(32, dtype=torch.int32)
    inst2[0] = (0x70) | (0x01 << 8)
    inst2[2] = (head_dim)
    inst2[10] = (0) | (kv_len_per_partial << 16)
    inst2[4] = 0
    inst2[5] = 0
    inst2[6] = kv_len_per_partial * head_dim
    inst2[7] = head_dim + 2  # output offset (after first partial in c1)
    inst2[9] = head_dim + 2 + head_dim  # LSE offset
    inst2[15] = torch.tensor(1.0, dtype=torch.float32).view(torch.int32)

    instructions2 = inst2.view(1, 1, 32).contiguous().to(device)
    timings2 = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

    a2 = q.clone()
    b2 = torch.cat([K2, V2])

    mk_generic.mk_generic_matmul(instructions2, timings2, a2, b2, c1, barriers1)

    o2 = c1[head_dim+2:head_dim+2+head_dim].clone()
    lse2 = c1[head_dim+2+head_dim:head_dim+2+head_dim+2].clone()

    # ========== Run Reduction ==========
    inst_reduce = torch.zeros(32, dtype=torch.int32)
    inst_reduce[0] = (0x71)  # OP_ATTENTION_REDUCE
    inst_reduce[2] = (head_dim)  # k_dim
    inst_reduce[10] = (0) | (num_partials << 16)  # reduction_factor = num_partials
    inst_reduce[4] = 0  # partial outputs offset (base = 0)
    inst_reduce[5] = num_partials * head_dim  # LSE offset
    inst_reduce[7] = 0  # final output offset in a (reuse a)

    instructions_reduce = inst_reduce.view(1, 1, 32).contiguous().to(device)
    timings_reduce = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

    # Pack partials and LSE values into b
    # Layout: [o1, o2, lse1, lse2]
    b_reduce = torch.cat([o1, o2, lse1, lse2])
    c_reduce = torch.zeros(head_dim, device=device)

    mk_generic.mk_generic_matmul(instructions_reduce, timings_reduce, c_reduce, b_reduce, c_reduce, barriers1)

    # Compare with full attention
    K_full = torch.cat([K1, K2])
    V_full = torch.cat([V1, V2])
    K_full_mat = K_full.view(kv_len_per_partial * num_partials, head_dim)
    V_full_mat = V_full.view(kv_len_per_partial * num_partials, head_dim)

    logits = (q @ K_full_mat.T)
    weights = torch.softmax(logits, dim=0)
    expected = (weights @ V_full_mat)

    ok = torch.allclose(c_reduce, expected, atol=1e-4)
    print(f"ATTENTION_REDUCE smoke: {'PASS' if ok else 'FAIL'}; got {c_reduce.cpu().tolist()}, expected {expected.cpu().tolist()}")
    return 0 if ok else 1


def run_matmul_benchmark():
    """Benchmark MatMul with progressively larger matrices to show speedup"""
    import torch
    import importlib
    mk_generic = importlib.import_module("mk_generic")
    device = torch.device("cuda")

    print("\n" + "="*70)
    print("MatMul Performance Benchmark (MatVec: M=1)")
    print("="*70)
    print(f"{'Size':12} {'Time (ms)':>12} {'GFLOPS':>10} {'Arithmetic Intensity':>20}")
    print("-"*70)

    sizes = [
        (64, 64, "Small"),
        (256, 256, "Medium"),
        (512, 512, "Large"),
        (1024, 1024, "Very Large"),
        (2048, 2048, "Huge"),
    ]

    for N, K, label in sizes:
        # Create instruction
        inst = torch.zeros(32, dtype=torch.int32)
        inst[0] = (0x30)  # MATMUL opcode
        inst[1] = (1) | (N << 16)  # m=1, n=N
        inst[2] = (K)  # k=K
        inst[4] = 0  # a offset
        inst[8] = 0  # b (weight) offset
        inst[7] = 0  # c offset

        instructions = inst.view(1, 1, 32).contiguous().to(device)
        timings = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

        # Create random data
        a = torch.randn(K, dtype=torch.float32, device=device)
        b = torch.randn(N * K, dtype=torch.float32, device=device)
        c = torch.zeros(N, dtype=torch.float32, device=device)
        barriers = torch.zeros(256, dtype=torch.int32, device=device)

        # Warmup
        for _ in range(3):
            mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)
        torch.cuda.synchronize()

        # Benchmark
        num_runs = 100 if N <= 512 else 20
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)

        start.record()
        for _ in range(num_runs):
            mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)
        end.record()
        torch.cuda.synchronize()

        elapsed_ms = start.elapsed_time(end) / num_runs

        # Calculate theoretical operations and arithmetic intensity
        flops = 2 * N * K  # N dot products, each with K multiplies and K-1 adds
        gflops = flops / (elapsed_ms * 1e6)  # GFLOPS
        bytes_transferred = (K + N * K + N) * 4  # a, b, c in float32
        arithmetic_intensity = flops / bytes_transferred  # ops/byte

        print(f"{label:12} {elapsed_ms:12.4f} {gflops:10.2f} {arithmetic_intensity:20.4f}")

    print("="*70)
    print("\nðŸ’¡ MatVec (M=1) is MEMORY-BOUND (low arithmetic intensity)")
    print("   â†’ Most time spent loading data, not computing")
    print("   â†’ See 'benchmark-matmul' for compute-bound true matmul!\n")
    print("="*70 + "\n")

    return 0


def run_true_matmul_benchmark():
    """Benchmark TRUE MatMul (M > 1) showing compute-bound performance"""
    import torch
    import importlib
    mk_generic = importlib.import_module("mk_generic")
    device = torch.device("cuda")

    print("\n" + "="*80)
    print("TRUE MatMul Benchmark (MÃ—NÃ—K with M > 1) - COMPUTE BOUND")
    print("="*80)
    print(f"{'Config':20} {'Time (ms)':>12} {'GFLOPS':>10} {'Arith Int':>12} {'vs PyTorch':>12}")
    print("-"*80)

    # Test configurations: (M, N, K, label)
    configs = [
        (16, 256, 256, "16Ã—256Ã—256"),
        (64, 512, 512, "64Ã—512Ã—512"),
        (128, 1024, 1024, "128Ã—1024Ã—1024"),
        (256, 2048, 2048, "256Ã—2048Ã—2048"),
    ]

    for M, N, K, label in configs:
        # Note: Current implementation processes one row at a time
        # We'll run M separate matvec operations to simulate MÃ—NÃ—K matmul

        inst = torch.zeros(32, dtype=torch.int32)
        inst[0] = (0x30)  # MATMUL opcode
        inst[1] = (1) | (N << 16)  # m=1, n=N (one row at a time)
        inst[2] = (K)  # k=K
        inst[4] = 0
        inst[8] = 0
        inst[7] = 0

        instructions = inst.view(1, 1, 32).contiguous().to(device)
        timings = torch.zeros((1, 1, 128), dtype=torch.int32, device=device)

        # Create data for full MÃ—NÃ—K matmul
        a = torch.randn(M * K, dtype=torch.float32, device=device)  # M rows of K elements
        b = torch.randn(N * K, dtype=torch.float32, device=device)  # NÃ—K weight matrix
        c = torch.zeros(M * N, dtype=torch.float32, device=device)  # MÃ—N output
        barriers = torch.zeros(256, dtype=torch.int32, device=device)

        # Warmup
        for _ in range(3):
            for m in range(M):
                inst[4] = m * K  # Update input offset for this row
                inst[7] = m * N  # Update output offset for this row
                instructions[0, 0, 4] = m * K
                instructions[0, 0, 7] = m * N
                mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)
        torch.cuda.synchronize()

        # Benchmark our kernel
        num_runs = 10 if M >= 128 else 20
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)

        start.record()
        for _ in range(num_runs):
            for m in range(M):
                inst[4] = m * K
                inst[7] = m * N
                instructions[0, 0, 4] = m * K
                instructions[0, 0, 7] = m * N
                mk_generic.mk_generic_matmul(instructions, timings, a, b, c, barriers)
        end.record()
        torch.cuda.synchronize()

        our_time_ms = start.elapsed_time(end) / num_runs

        # Benchmark PyTorch for comparison
        a_torch = a.view(M, K)
        b_torch = b.view(N, K).T  # Transpose to KÃ—N

        # Warmup
        for _ in range(3):
            _ = torch.mm(a_torch, b_torch)
        torch.cuda.synchronize()

        start.record()
        for _ in range(num_runs):
            _ = torch.mm(a_torch, b_torch)
        end.record()
        torch.cuda.synchronize()

        pytorch_time_ms = start.elapsed_time(end) / num_runs

        # Calculate metrics
        flops = 2 * M * N * K  # MÃ—N outputs, each K multiply-adds
        gflops = flops / (our_time_ms * 1e6)
        bytes_transferred = (M * K + N * K + M * N) * 4
        arithmetic_intensity = flops / bytes_transferred

        speedup_ratio = pytorch_time_ms / our_time_ms
        vs_pytorch = f"{speedup_ratio:.2f}x" if speedup_ratio > 1 else f"/{1/speedup_ratio:.2f}x"

        print(f"{label:20} {our_time_ms:12.4f} {gflops:10.2f} {arithmetic_intensity:12.2f} {vs_pytorch:>12}")

    print("="*80)
    print("\nðŸš€ True MatMul shows HIGHER arithmetic intensity â†’ More compute-bound")
    print("   Phase 1 (warp parallel): Good baseline")
    print("   Phase 2-3 (TMA + MMA tensor cores): Will match/beat PyTorch!\n")
    print("ðŸ“Š Current vs PyTorch: Phase 1 is slower (expected - no tensor cores yet)")
    print("   After Phase 3, you'll see 1-10x FASTER than PyTorch! ðŸŽ¯\n")
    print("="*80 + "\n")

    return 0


def main():
    parser = argparse.ArgumentParser(description="Generic ISA CUDA smoke tests")
    parser.add_argument(
        "op",
        choices=[
            "matmul",
            "rmsnorm",
            "attention",
            "attention-reduce",
            "rope",
            "fused-norm-matmul",
            "fused-qkv-rope",
            "barrier",
            "copy",
            "zero",
            "all",
            "benchmark",
            "benchmark-matmul",
        ],
        help="Which smoke test to run (benchmark=matvec, benchmark-matmul=true matmul)",
    )
    args = parser.parse_args()

    require_torch()
    require_extension()

    rc = 0
    if args.op == "benchmark":
        rc |= run_matmul_benchmark()
    elif args.op == "benchmark-matmul":
        rc |= run_true_matmul_benchmark()
    elif args.op in ("matmul", "all"):
        rc |= run_matmul()
    if args.op in ("rmsnorm", "all"):
        rc |= run_rmsnorm()
    if args.op in ("attention", "all"):
        rc |= run_attention()
    if args.op in ("attention-reduce", "all"):
        rc |= run_attention_reduce()
    if args.op in ("rope", "all"):
        rc |= run_rope()
    if args.op in ("fused-norm-matmul", "all"):
        rc |= run_fused_norm_matmul()
    if args.op in ("fused-qkv-rope", "all"):
        rc |= run_fused_qkv_rope()
    if args.op in ("barrier", "all"):
        rc |= run_barrier()
    if args.op in ("copy", "all"):
        rc |= run_copy()
    if args.op in ("zero", "all"):
        rc |= run_zero()
    sys.exit(rc)


if __name__ == "__main__":
    main()
