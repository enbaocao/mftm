# Generic Hopper Instruction Set Architecture

## Overview

This document describes the **Generic Instruction Set Architecture (ISA)** for NVIDIA Hopper (H100) and Blackwell (B200) GPUs. Unlike the model-specific instruction sets in `demos/low-latency-llama`, this ISA is designed to work across different transformer architectures without recompilation.

## Motivation

The original megakernel implementation is tightly coupled to specific model architectures:
- **Compile-time dimensions**: Model parameters (hidden_dim, num_heads, etc.) are template parameters
- **Fixed opcodes**: Each demo defines its own opcode scheme (7 opcodes for Llama latency mode, 10 for throughput)
- **Model-specific kernels**: `llama.cu` hardcodes Llama-1B dimensions

This limits reusability. To run a different model size or architecture, you must:
1. Modify C++ template parameters
2. Recompile CUDA kernels (5-10 minutes)
3. Create new Python bindings

**The Generic ISA solves this** by moving configuration to runtime while maintaining peak performance.

## Design Principles

1. **Runtime Configuration**: Model dimensions specified at runtime, not compile-time
2. **Primitive Operations**: ~20 fundamental opcodes that compose into any transformer
3. **Flexible Dispatch**: Single compiled kernel handles multiple model architectures
4. **Performance**: <10% overhead vs. hand-tuned specialized kernels
5. **Portability**: Works on both Hopper (H100) and Blackwell (B200) with architecture-specific optimizations

## Architecture Components

### 1. Opcodes (`include/generic/opcodes.cuh`)

Defines 256 possible opcodes organized into categories:

```cpp
// Memory operations (0x10-0x2F)
OP_LOAD_ACTIVATION, OP_STORE_ACTIVATION, OP_CACHE_APPEND

// Linear algebra (0x30-0x4F)
OP_MATMUL, OP_MATVEC, OP_BATCH_MATMUL

// Normalization (0x50-0x6F)
OP_RMS_NORM, OP_LAYER_NORM

// Attention (0x70-0x8F)
OP_ATTENTION_PARTIAL, OP_ATTENTION_REDUCE, OP_ROPE_EMBED

// Activations (0x90-0x9F)
OP_SILU, OP_GELU, OP_RELU, OP_SWIGLU

// Element-wise (0xA0-0xAF)
OP_ADD, OP_RESIDUAL_ADD

// Fused operations (0xB0-0xCF) - for performance
OP_FUSED_NORM_MATMUL, OP_FUSED_ROPE_APPEND, OP_FUSED_QKV_PROJ
```

**Key Feature**: Fused operations allow specialized kernels for common patterns (e.g., `FUSED_NORM_QKV_ROPE` combines RMSNorm + QKV projection + RoPE in one kernel for Llama).

### 2. Runtime Model Configuration (`include/generic/model_config.cuh`)

Replaces compile-time templates with runtime config:

```cpp
struct RuntimeModelConfig {
    // Architecture
    uint16_t num_layers, hidden_dim, intermediate_dim, vocab_size;

    // Attention
    uint16_t num_q_heads, num_kv_heads, head_dim;  // GQA/MQA support
    uint16_t attn_config;  // Encodes MHA/GQA/MQA/MLA + RoPE/ALiBi + masking

    // MLP
    ActivationType mlp_activation;  // SiLU, GELU, etc.
    bool mlp_gated;  // SwiGLU vs standard MLP

    // Normalization
    NormType norm_type;  // RMSNorm, LayerNorm
    float norm_eps;

    // Tiling parameters (tuned per GPU)
    uint16_t matmul_block_m, matmul_block_n, matmul_block_k;

    // Hardware
    uint16_t sm_count;
    bool is_hopper, is_blackwell;
};
```

**Presets provided**:
- `make_llama_3_1b_config()` - Llama 3.2 1B
- `make_gpt2_config()` - GPT-2 124M
- `make_mistral_7b_config()` - Mistral 7B with sliding window attention

### 3. Generic Instruction Format (`include/generic/instruction.cuh`)

64-byte instruction structure (fits in 2 cache lines):

```cpp
struct GenericInstruction {
    uint8_t opcode;
    uint8_t flags;           // Transpose, accumulate, etc.
    uint16_t layer_idx;

    // Runtime dimensions
    uint16_t m_dim, n_dim, k_dim;
    uint16_t block_idx_m, block_idx_n, block_idx_k;

    // Memory addressing (element offsets)
    uint32_t input_offset_0, input_offset_1, input_offset_2;
    uint32_t output_offset, weight_offset, scratch_offset;

    // Configuration
    uint16_t head_config;    // Attention pattern
    uint16_t reduction_factor;
    uint16_t seq_pos, batch_idx;

    // Synchronization
    uint16_t dependency_mask;
    uint8_t sync_slot, sync_count;

    // Metadata
    uint32_t instruction_id;
    float scale_factor;      // For attention scaling, etc.
};
```

**Builder helpers** provided for common patterns:
- `make_matmul_instruction()`
- `make_norm_instruction()`
- `make_attention_partial_instruction()`
- `make_fused_norm_matmul_instruction()`

### 4. Runtime Globals (`include/generic/globals.cuh`)

Unified memory layout supporting dynamic dimensions:

```cpp
template <typename config>
struct RuntimeGlobals {
    RuntimeModelConfig model_cfg;

    // Unified weight buffers (all layers)
    gl<bf16, -1, -1, -1, -1> unified_weights;
    gl<bf16, -1, -1, -1> norm_weights;

    // KV cache (shape: num_layers, 2, num_kv_heads, max_seq_len, head_dim)
    gl<bf16, -1, -1, -1, -1, -1> kv_cache;

    // Position embeddings
    gl<float, -1, -1> rope_cos, rope_sin;
    gl<bf16, -1, -1> pos_embed_table;  // For GPT-2
    gl<float, -1> alibi_slopes;        // For ALiBi

    // Activation buffers
    gl<bf16, -1, -1> hidden_states, residual_states;
    gl<bf16, -1, -1, -1> q_proj, k_proj, v_proj, attn_output;
    gl<bf16, -1, -1> gate_output, up_output, mlp_output;
    gl<bf16, -1, -1> logits;
};
```

### 5. Python Scheduler (`megakernels/generic_scheduler.py`)

Model-agnostic instruction generation:

```python
class UniversalScheduler:
    def __init__(self, model_cfg: ModelConfig):
        self.cfg = model_cfg

    def build_transformer_layer(self, layer_idx: int) -> List[GenericInstruction]:
        """Generate instructions for one transformer layer"""
        # Automatically adapts to model architecture

    def build_full_model(self) -> List[GenericInstruction]:
        """Generate complete instruction sequence"""
```

**Supports multiple architectures out of the box**:
- **Llama**: RMSNorm + GQA + RoPE + SwiGLU
- **GPT-2**: LayerNorm + MHA + learned position embeddings + GELU
- **Mistral**: RMSNorm + GQA + RoPE + sliding window attention + SwiGLU

## Example Usage

### Python Side: Generate Instructions

```python
from megakernels.generic_scheduler import ModelConfig, UniversalScheduler

# Create config for your model
config = ModelConfig.from_llama_3_1b(sm_count=132)
# Or: config = ModelConfig.from_gpt2()
# Or: config = ModelConfig.from_mistral_7b()

# Generate instructions
scheduler = UniversalScheduler(config)
instructions = scheduler.build_full_model(use_fused_ops=True)

print(f"Generated {len(instructions)} instructions")
print(f"First instruction: {Opcode(instructions[0].opcode).name}")

# Serialize for GPU
instruction_buffer = np.stack([inst.serialize() for inst in instructions])
```

### C++ Side: Dispatch Instructions

```cpp
#include "generic/generic.cuh"

using namespace megakernel::generic;

// Create runtime configuration
RuntimeModelConfig cfg = make_llama_3_1b_config();
PerformanceHints::detect_hardware(cfg);  // Auto-detect H100 vs B200

// Allocate buffers
RuntimeGlobals<default_config> globals;
allocate_buffers(globals, cfg, "cuda:0");

// Decode and execute instruction
GenericInstruction inst = decode_instruction(instruction_buffer);

switch (inst.opcode) {
case OP_MATMUL:
    dispatch_matmul_op(globals, mks, inst);
    break;
case OP_ATTENTION_PARTIAL:
    dispatch_attention_partial_op(globals, mks, inst);
    break;
// ... other opcodes
}
```

## Attention Pattern Support

The `head_config` field encodes different attention patterns:

```cpp
// Create attention config
uint16_t config = make_attn_config(
    ATTN_TYPE_GQA,           // Grouped Query Attention
    POS_EMB_ROPE,            // RoPE position embeddings
    MASK_CAUSAL,             // Causal masking
    ATTN_FEAT_FLASH          // Flash attention style
);

// Query at runtime
if (get_attn_type(config) == ATTN_TYPE_GQA) {
    // Use GQA kernel
    int num_groups = num_q_heads / num_kv_heads;
}
```

**Supported patterns**:
- **MHA** (Multi-Head Attention): All models have separate K/V per head
- **GQA** (Grouped Query Attention): Multiple Q heads share K/V heads (Llama 3, Mistral)
- **MQA** (Multi-Query Attention): All Q heads share one K/V head (some efficient models)
- **MLA** (Multi-Latent Attention): Compressed latent attention (DeepSeek-V2)

**Position embeddings**:
- **RoPE**: Rotary position embeddings (Llama, Mistral, etc.)
- **RoPE Partial**: Only rotate first N dimensions (Qwen)
- **ALiBi**: Attention with Linear Biases
- **Learned**: Absolute position embeddings (GPT-2)

**Masking**:
- **Causal**: Standard autoregressive masking
- **Sliding Window**: Local attention with window (Mistral)
- **Block Sparse**: Block-structured sparsity

## Performance Characteristics

### Expected Performance vs. Specialized Kernels

| Model | Specialized Kernel | Generic ISA | Overhead |
|-------|-------------------|-------------|----------|
| Llama 3.2 1B | 100% | 95-98% | 2-5% |
| GPT-2 124M | 100% | 93-97% | 3-7% |
| Mistral 7B | 100% | 94-97% | 3-6% |

**Overhead sources**:
1. **Runtime dispatch** (1-2%): Opcode switch vs. compile-time dispatch
2. **Less aggressive inlining** (1-2%): Generic kernels can't inline as aggressively
3. **Dimension checks** (0-1%): Runtime validation of dimensions

**Mitigation strategies**:
1. **Fused operations**: `FUSED_NORM_QKV_ROPE` combines multiple ops
2. **Template specialization**: Specialize for common dimension combinations
3. **Constexpr dispatch**: Use C++20 `constexpr if` to eliminate branches
4. **Profile-guided optimization**: JIT compile hot paths

### Hopper vs. Blackwell

The ISA is designed for portability:

```cpp
if (cfg.is_blackwell) {
    // B200 optimizations
    cfg.matmul_block_k *= 2;       // More shared memory
    cfg.attn_block_kv = 128;       // vs 64 on H100
    use_5th_gen_tensor_cores();    // New instructions
}
```

**Expected Blackwell speedups**:
- **1.5-2x** from 5th gen tensor cores
- **1.2-1.3x** from larger shared memory (232KB vs 228KB)
- **1.1-1.2x** from higher SM count (148 vs 132)
- **Combined**: ~2-2.5x total

## Implementation Status

### âœ… Completed (Phase 1-2)

**Core Infrastructure:**
- [x] Core opcode definitions (~20 opcodes) - `include/generic/opcodes.cuh`
- [x] Runtime model configuration structure - `include/generic/model_config.cuh`
- [x] Generic instruction format (64 bytes) - `include/generic/instruction.cuh`
- [x] Runtime globals with dynamic dimensions - `include/generic/globals.cuh`
- [x] Python scheduler for multi-model support - `megakernels/generic_scheduler.py`
- [x] Model configuration presets (Llama, GPT-2, Mistral)
- [x] Attention pattern encoding (MHA/GQA/MQA)
- [x] Builder helpers for common instructions

**Operations Implemented:**
- [x] OP_BARRIER, OP_SYNC - Synchronization primitives (`include/generic/ops/barrier.cuh`)
- [x] OP_COPY, OP_ZERO - Memory operations (`include/generic/ops/memory.cuh`)
- [x] OP_MATMUL - Matrix multiplication (naive, single-threaded for smoke tests)
- [x] OP_RMS_NORM, OP_LAYER_NORM - Normalization ops (naive implementation)
- [x] OP_ATTENTION_PARTIAL - Attention with LSE output support
- [x] OP_ATTENTION_REDUCE - Multi-partial attention reduction
- [x] OP_ROPE_EMBED - Rotary position embeddings
- [x] OP_FUSED_NORM_MATMUL, OP_FUSED_NORM_QKV_ROPE - Fused operations (smoke behavior)

**Testing:**
- [x] 24 CPU-based unit tests passing (no CUDA dependencies)
- [x] CUDA smoke tests for all compute ops (MATMUL, RMS_NORM, LAYER_NORM, ATTENTION_PARTIAL, ATTENTION_REDUCE, ROPE_EMBED, FUSED ops)
- [x] Python scheduler tested with Llama 3.2 1B, GPT-2 124M, Mistral 7B

**Demonstration:**
Running `python3 megakernels/generic_scheduler.py` shows successful instruction generation:
- Llama 3.2 1B: 8 instructions/layer, 41.97M FLOPs/layer
- GPT-2 124M: 9 instructions/layer, 1.19M FLOPs/layer
- Mistral 7B: 8 instructions/layer, 151.05M FLOPs/layer

**Key Innovation**: Flexible encoding allows single opcode to handle multiple patterns:
- `OP_ATTENTION_PARTIAL` supports MHA, GQA, MQA, and MLA via `head_config` field
- Flags enable modifiers like transpose, accumulate, residual addition
- 256 opcode space with room for model-specific extensions

### ðŸš§ In Progress (Phase 3-4)

**Performance Optimization (CRITICAL):**
- [ ] ThunderKittens tile-based MatMul (replace naive scalar loops) - **See `TK_OPTIMIZATION_PLAN.md`**
- [ ] ThunderKittens warp reductions for Norm ops
- [ ] Flash Attention pattern with TK tiles
- [ ] TMA async loads for memory operations
- [ ] Multi-warp parallelism for all ops

**Current Performance Issue:**
âš ï¸ **Ops are ~1000x slower than needed** - Current implementations use single-threaded scalar loops (1 thread doing all work, 895 threads idle). Example: 2048Ã—2048 matmul takes ~4 seconds vs ~4ms with ThunderKittens tiles.

**Integration:**
- [ ] Consumer/Loader/Storer dispatch for generic opcodes
- [ ] Controller instruction fetch for generic format
- [ ] Megakernel main loop integration

### ðŸ“‹ Planned (Phase 5-6)

- [ ] Blackwell-specific optimizations
- [ ] Auto-tuning for block sizes
- [ ] Correctness validation (Llama, GPT-2, Mistral vs PyTorch)
- [ ] Performance benchmarking and optimization
- [ ] Support for Mamba, RWKV (non-attention models)
- [ ] FP8 support for inference
- [ ] Multi-GPU tensor parallelism

## Next Steps

1. **Implement core operation kernels** using ThunderKittens primitives
2. **Create dispatch infrastructure** to route opcodes to kernels
3. **Integrate with existing megakernel framework** (consumer/loader/storer warps)
4. **Validate correctness** against PyTorch reference implementations
5. **Benchmark performance** and optimize hot paths
6. **Add Blackwell support** and test on B200

## Architecture Highlights

### 1. Zero-Recompilation Model Changes

```bash
# OLD: Change model size
# 1. Edit llama.cuh templates (change hidden_dim, num_heads, etc.)
# 2. make clean && make  (5-10 minutes)
# 3. Update Python bindings
# 4. pip install -e .

# NEW: Change model
# 1. Update runtime config (2 lines of Python)
config.hidden_dim = 4096
config.num_q_heads = 32
# Done! Same kernel binary works.
```

### 2. Fused Operations for Performance

Critical paths use fused kernels to minimize memory traffic:

```cpp
// Instead of separate: RMSNorm -> QKV MatMul -> RoPE (3 kernel launches, 3x memory)
// Use fused: FUSED_NORM_QKV_ROPE (1 kernel launch, 1x memory)

GenericInstruction fused = make_fused_norm_qkv_rope_instruction(
    layer_idx, m_dim, n_dim, k_dim,
    input_offset, norm_weight_offset, matmul_weight_offset, output_offset
);
// ~20-30% faster due to data reuse in shared memory
```

### 3. Hardware Portability (Hopper â†’ Blackwell)

Architecture detection and optimization:

```cpp
PerformanceHints::detect_hardware(cfg);

if (cfg.is_blackwell) {
    // B200 optimizations
    cfg.matmul_block_k *= 2;       // More shared memory (232KB vs 228KB)
    cfg.attn_block_kv = 128;       // vs 64 on H100
    use_5th_gen_tensor_cores();    // New instructions
}
```

Expected speedups on B200:
- 1.5-2x from 5th gen tensor cores
- 1.2x from shared memory increase
- 1.1x from higher SM count (148 vs 132)
- **~2-2.5x total**

## File Structure

```
include/generic/
â”œâ”€â”€ opcodes.cuh          (~350 lines) - Opcode definitions
â”œâ”€â”€ model_config.cuh     (~380 lines) - Runtime configuration
â”œâ”€â”€ instruction.cuh      (~350 lines) - Instruction format
â”œâ”€â”€ globals.cuh          (~280 lines) - Runtime globals
â”œâ”€â”€ generic.cuh          (~250 lines) - Main header
â”œâ”€â”€ ops/
â”‚   â”œâ”€â”€ barrier.cuh      - Barrier and sync operations
â”‚   â”œâ”€â”€ memory.cuh       - Copy and zero operations
â”‚   â”œâ”€â”€ matmul.cuh       - Matrix multiplication (naive)
â”‚   â”œâ”€â”€ norm.cuh         - RMS/Layer normalization (naive)
â”‚   â”œâ”€â”€ attention.cuh    - Attention partial and reduce
â”‚   â”œâ”€â”€ rope.cuh         - Rotary position embeddings
â”‚   â””â”€â”€ fused.cuh        - Fused operation kernels
â””â”€â”€ README.md            (~350 lines) - API documentation

megakernels/
â””â”€â”€ generic_scheduler.py (~650 lines) - Python scheduler

demos/
â””â”€â”€ generic-hopper/      - CUDA demo and smoke tests
    â”œâ”€â”€ Makefile
    â”œâ”€â”€ generic_kernel.cu
    â””â”€â”€ README.md

tests/generic/
â”œâ”€â”€ test_barrier_and_memory.py  - 16 unit tests
â”œâ”€â”€ test_reference_ops.py       - CPU reference implementations
â”œâ”€â”€ test_scheduler_*.py         - Scheduler validation
â””â”€â”€ test_smoke_demo.py          - CUDA smoke tests

**Total code**: ~3,000 lines of C++ headers + 650 lines of Python + comprehensive tests
```

## References

- **ThunderKittens**: `ThunderKittens/README.md`
- **Megakernel Architecture**: `include/megakernel.cuh`
- **Existing Llama Demo**: `demos/low-latency-llama/`
- **Flash Attention**: Dao et al., "FlashAttention: Fast and Memory-Efficient Exact Attention"
- **GQA**: Ainslie et al., "GQA: Training Generalized Multi-Query Transformer Models"

## Contributing

To add support for a new model architecture:

1. Define `RuntimeModelConfig` preset in `model_config.cuh`
2. If needed, add new opcodes to `opcodes.cuh` (use 0xF0-0xFF range)
3. Implement operation kernel if unique (most models use existing ops)
4. Add Python factory method in `generic_scheduler.py`
5. Test correctness against PyTorch reference

Expected time to add new architecture: **<48 hours** (vs. weeks for specialized implementation).
